{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "James Gardner, April 2022 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want to analyse science case/s for CE only:\n",
    "\n",
    "CE-N 40km with CE-S 40km or 20km\n",
    "\n",
    "if done, then look at CE-S with one ET detector\n",
    "\n",
    "verify techniques by replicating *Borhanian and Sathya 2022*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from networks import *\n",
    "from useful_functions import *\n",
    "from useful_plotting_functions import *\n",
    "from network_subclass import NetworkExtended\n",
    "from results_class import InjectionResults\n",
    "from calculate_injections import *\n",
    "from cosmological_redshift_resampler import *\n",
    "from plot_collated_detection_rate import *\n",
    "from plot_collated_PDFs_and_CDFs import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# suppress warnings\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicating Borhanian and Sathya 2022 injections and detection rates, then for CE only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch detection rate results for given science case and network set\n",
    "\n",
    "# waveform, LAL list: https://lscsoft.docs.ligo.org/lalsuite/lalsimulation/group___l_a_l_sim_inspiral__h.html\n",
    "# --- BNS ---\n",
    "# science_case = 'BNS'\n",
    "# wf_model_name, wf_other_var_dic = 'lal_bns', dict(approximant='IMRPhenomD_NRTidalv2') # for tidal, see https://arxiv.org/abs/1905.06011\n",
    "# to-do: missing dimensionless tidal parameters, calculate tidal parameters from sampled m1, m2 in injections.py? requires Love number and radii (i.e. choose an EoS)\n",
    "# --- BBH ---\n",
    "science_case = \"BBH\"\n",
    "wf_model_name, wf_other_var_dic = \"lal_bbh\", dict(approximant=\"IMRPhenomHM\")\n",
    "# --- analytic waveforms ---\n",
    "# wf_model_name, wf_other_var_dic = 'tf2', None # to-do: stop using this once tidal params found\n",
    "# wf_model_name, wf_other_var_dic = 'tf2_tidal', None\n",
    "\n",
    "# number of injections per redshift bin (6 bins)\n",
    "num_injs = 10\n",
    "\n",
    "network_spec_list = BS2022_SIX[\"nets\"]\n",
    "# network_spec_list = CE_ONLY['nets']\n",
    "# network_spec_list = CE_S_W_ET[\"nets\"]\n",
    "\n",
    "for network_spec in tqdm(network_spec_list):\n",
    "    detection_rate_for_network_and_waveform(\n",
    "        network_spec,\n",
    "        science_case,\n",
    "        wf_model_name,\n",
    "        wf_other_var_dic,\n",
    "        num_injs,\n",
    "        show_fig=False,\n",
    "        print_progress=False,\n",
    "        print_reach=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-batch to save all the required results (but not generate plots to save time) for the below cell\n",
    "network_spec_list = [\n",
    "    NET_DICT_LIST[0][\"nets\"][5],\n",
    "]  # NET_LIST\n",
    "\n",
    "science_cases = (\"BNS\", \"BBH\")\n",
    "# --- numerical BNS as in B&S2021 ---\n",
    "# wf_specifications = (('lal_bns', dict(approximant='IMRPhenomD_NRTidalv2')), ('lal_bbh', dict(approximant='IMRPhenomHM')))\n",
    "# --- analytic BNS ---\n",
    "wf_specifications = ((\"tf2_tidal\", None), (\"lal_bbh\", dict(approximant=\"IMRPhenomHM\")))\n",
    "\n",
    "# number of injections per redshift bin (6 bins), tuple over science cases (more for symbolic wf.'s than numerical)\n",
    "num_injs_list = (60000, 60000)\n",
    "\n",
    "for network_spec in tqdm(network_spec_list):\n",
    "    for j, science_case in enumerate(science_cases):\n",
    "        wf_model_name, wf_other_var_dic = wf_specifications[j]\n",
    "        detection_rate_for_network_and_waveform(\n",
    "            network_spec,\n",
    "            science_case,\n",
    "            wf_model_name,\n",
    "            wf_other_var_dic,\n",
    "            num_injs_list[j],\n",
    "            generate_fig=True,\n",
    "            show_fig=False,\n",
    "            print_progress=False,\n",
    "            print_reach=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# debugging m1 <= 0 error\n",
    "\n",
    "# run benchmarking on a single, manually set injection that caused the error\n",
    "network_spec = [\"A+_H\", \"A+_L\", \"V+_V\", \"K+_K\", \"A+_I\"]\n",
    "science_case, wf_model_name, wf_other_var_dic = (\n",
    "    \"BBH\",\n",
    "    \"lal_bbh\",\n",
    "    dict(approximant=\"IMRPhenomHM\"),\n",
    ")\n",
    "num_injs = 1  # arb, will override\n",
    "file_name = \"test.npy\"\n",
    "data_path = \"/fred/oz209/jgardner/CEonlyPony/source/data_redshift_snr_errs_sky-area/\"\n",
    "\n",
    "detection_rate_for_network_and_waveform(\n",
    "    network_spec,\n",
    "    science_case,\n",
    "    wf_model_name,\n",
    "    wf_other_var_dic,\n",
    "    num_injs,\n",
    "    generate_fig=False,\n",
    "    show_fig=False,\n",
    "    print_progress=False,\n",
    "    print_reach=False,\n",
    "    data_path=data_path,\n",
    "    file_name=file_name,\n",
    "    parallel=False,\n",
    "    use_BS2022_seeds=False,\n",
    "    log_uniformly_sampled_redshift=False,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "# hfp, hfc created before errors, therefore problem with net.calc_wf_polarizations_derivs_num?\n",
    "#         self.del_hfpc = drd.calc_det_responses_derivs_num(None,self.wf,wf_deriv_symbs_string,self.f,self.inj_params,self.use_rot,'hf',step,method,order,n)\n",
    "# eventually: gwbench wf_derivatives_num.part_deriv_ndGradient\n",
    "# numerical gradient iterates and errors when m1=m2=nan when eta = 0.25000000071132467 occurs, is eta changing to get the gradient along eta evaluated at eta_0?\n",
    "# eta is maximised at 0.25 when m1 = m2, since eta = 0.25 + epsilon, epsilon > 0, m1 and m2 error out\n",
    "# the problem: there is no safeguard in wf_derivatives_num.part_deriv_ndGradient against eta close to 0.25, need to manually check step size (1e-9 too large)!\n",
    "# eta can be close to 0.25 because m2 is sampled in [5 Msun, m1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating different networks saved using the above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch of six collated plots: all three network sets and two science cases\n",
    "# assumes results saved: above results generating cell must be run to save results for this cell\n",
    "\n",
    "# ! remember to update plot label for each case else the plot will be overwritten\n",
    "# how standard the \"standard 6\" are is unclear, seems like an invention of B&S2022\n",
    "# CE only, with 2G+ as a reference\n",
    "# CE South with one ET detector\n",
    "\n",
    "# adding HLVKI+ reference to CE nets\n",
    "network_dict_list = [\n",
    "    NET_DICT_LIST[0],\n",
    "]\n",
    "network_sets = [net_dict[\"nets\"] for net_dict in network_dict_list]\n",
    "# (BS2022_STANDARD_6['nets'],)\n",
    "#                 CE_ONLY_C_and_S['nets'] + [['A+_H', 'A+_L', 'V+_V', 'K+_K', 'A+_I']],\n",
    "#                 CE_ONLY_C_and_N['nets'] + [['A+_H', 'A+_L', 'V+_V', 'K+_K', 'A+_I']],\n",
    "#                 CE_S_W_ET['nets'] + [['A+_H', 'A+_L', 'V+_V', 'K+_K', 'A+_I']])\n",
    "network_labels = [net_dict[\"label\"] for net_dict in network_dict_list]\n",
    "# ('standard_6', 'CE_only_C..S', 'CE_only_C..N', 'ET..CE_S')\n",
    "science_cases = (\"BNS\", \"BBH\")\n",
    "\n",
    "for i, network_set in enumerate(network_sets):\n",
    "    for j, science_case in enumerate(science_cases):\n",
    "        if science_case == \"BNS\":\n",
    "            # wf_model_name, wf_other_var_dic = 'lal_bns', dict(approximant='IMRPhenomD_NRTidalv2')\n",
    "            wf_model_name, wf_other_var_dic = (\n",
    "                \"tf2_tidal\",\n",
    "                None,\n",
    "            )  # to-do: change to more accurate numerical once gwbench patch released\n",
    "        elif science_case == \"BBH\":\n",
    "            wf_model_name, wf_other_var_dic = \"lal_bbh\", dict(approximant=\"IMRPhenomHM\")\n",
    "        else:\n",
    "            raise ValueError(\"Science case not recognised.\")\n",
    "\n",
    "        if wf_other_var_dic is not None:\n",
    "            plot_label = f'NET_{network_labels[i]}_SCI-CASE_{science_case}_WF_{wf_model_name}_{wf_other_var_dic[\"approximant\"]}'\n",
    "        else:\n",
    "            plot_label = (\n",
    "                f\"NET_{network_labels[i]}_SCI-CASE_{science_case}_WF_{wf_model_name}\"\n",
    "            )\n",
    "        compare_detection_rate_of_networks_from_saved_results(\n",
    "            network_set,\n",
    "            science_case,\n",
    "            plot_label=plot_label,\n",
    "            show_fig=False,\n",
    "            data_path=\"data_redshift_snr_errs_sky-area/\",\n",
    "            parallel=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B&S2022 plots cumulative density functions (CDF) versus redshift to show distribution of measurement errors (why not just use PDF, i.e. normalised histograms, like B2021?)\n",
    "# for non-SNR quantities only use results above an SNR threshold (e.g. 10), alternatively, for all non--sky area quantities only use results below a 90% credible sky area threshold (e.g. 4.4e-2 sqrDeg, where the full Moon is ~20.0e-2 sqrDeg given a radius of 0.5 deg) --> don't have enough resolution to attempt sky-area thresholding\n",
    "\n",
    "# batching collation of standard sets assuming results saved\n",
    "# compare plots to B&S2022 Figs 3 and 4\n",
    "network_dict_list = [\n",
    "    NET_DICT_LIST[0],\n",
    "]\n",
    "network_sets = [net_dict[\"nets\"] for net_dict in network_dict_list]\n",
    "network_labels = [net_dict[\"label\"] for net_dict in network_dict_list]\n",
    "# network_sets = (BS2022_STANDARD_6['nets'],\n",
    "#                 CE_ONLY_C_and_S['nets'] + [['A+_H', 'A+_L', 'V+_V', 'K+_K', 'A+_I']],\n",
    "#                 CE_ONLY_C_and_N['nets'] + [['A+_H', 'A+_L', 'V+_V', 'K+_K', 'A+_I']],\n",
    "#                 CE_S_W_ET['nets'] + [['A+_H', 'A+_L', 'V+_V', 'K+_K', 'A+_I']])\n",
    "# network_labels = ('standard_6', 'CE_only_C..S', 'CE_only_C..N', 'ET_E..CE_S')\n",
    "science_cases = (\"BNS\", \"BBH\")\n",
    "specific_wfs = (\"tf2_tidal\", None)\n",
    "\n",
    "for i, network_set in enumerate(tqdm(network_sets)):\n",
    "    for j, science_case in enumerate(science_cases):\n",
    "        plot_label = f\"SCI-CASE_{science_case}_NETS_{network_labels[i]}\"\n",
    "        plot_title = f\"Science case: {science_case}, network set: {network_labels[i]}\"\n",
    "        specific_wf = specific_wfs[j]\n",
    "        if specific_wf is not None:\n",
    "            plot_label += f\"_WF_{specific_wf}\"\n",
    "            plot_title += f\", WF: {specific_wf}\"\n",
    "        collate_measurement_errs_CDFs_of_networks(\n",
    "            network_set,\n",
    "            science_case,\n",
    "            specific_wf=specific_wf,\n",
    "            plot_label=plot_label,\n",
    "            plot_title=plot_title,\n",
    "            full_legend=False,\n",
    "            print_progress=False,\n",
    "            show_fig=False,\n",
    "            normalise_count=True,\n",
    "            xlim_list=None,\n",
    "            threshold_by_SNR=True,\n",
    "            CDFmin=1e-4,\n",
    "            data_path=\"data_redshift_snr_errs_sky-area/\",\n",
    "        )\n",
    "#         # for reference, without SNR thresholding\n",
    "#         collate_measurement_errs_CDFs_of_networks(network_set, science_case, specific_wf=specific_wf, plot_label=plot_label+'_SNR-THR_False', plot_title=plot_title, full_legend=False, print_progress=False, show_fig=False, normalise_count=True, xlim_list=None, threshold_by_SNR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for direct comparison, use B&S2022's xlim_list\n",
    "# --- options ---\n",
    "options = (\n",
    "    (BS2022_SIX, \"BBH\", \"lal_bbh_IMRPhenomHM\"),\n",
    "    (BS2022_SIX, \"BNS\", \"tf2_tidal\"),\n",
    ")\n",
    "\n",
    "for net_dict, science_case, wf_model_name in options:\n",
    "    network_set, network_label = net_dict[\"nets\"], net_dict[\"label\"]\n",
    "    plot_label = f\"NET_{network_label}_SCI-CASE_{science_case}_WF_{wf_model_name}\"\n",
    "    # plot_label = 'contoured'\n",
    "    plot_title = f\"Networks: {network_label}, science-case: {science_case}, waveform: {wf_model_name}\"\n",
    "    ymin_CDF = 1e-4\n",
    "    data_path = (\n",
    "        \"data_redshift_snr_errs_sky-area/\"  #'old_data_redshift_snr_errs_sky-area/'\n",
    "    )\n",
    "    linestyles_from_BS2022 = True\n",
    "\n",
    "    collate_measurement_errs_CDFs_of_networks(\n",
    "        network_set,\n",
    "        science_case,\n",
    "        plot_label=plot_label + \"_XLIMS_preset\",\n",
    "        plot_title=plot_title + \", XLIMS: preset to B&S2022\",\n",
    "        full_legend=False,\n",
    "        print_progress=False,\n",
    "        show_fig=False,\n",
    "        normalise_count=True,\n",
    "        xlim_list=\"B&S2022\",\n",
    "        threshold_by_SNR=True,\n",
    "        CDFmin=ymin_CDF,\n",
    "        data_path=data_path,\n",
    "        num_bins=40,\n",
    "        linestyles_from_BS2022=linestyles_from_BS2022,\n",
    "        contour=False,\n",
    "        parallel=True,\n",
    "        debug=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sky localisation\n",
    "Is there a better way to analyse and/or communicate sky localisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- input parameters ---\n",
    "data_path = \"tmp_data/data_redshift_snr_errs_sky-area/\"\n",
    "input_file_name = \"results_NET_CE-40-CBO_C..CE-20-PMO_N..CE-40-CBO_S_SCI-CASE_BNS_WF_tf2_tidal_INJS-PER-ZBIN_30000.npy\"\n",
    "plot_title = file_name_to_multiline_readable(input_file_name)\n",
    "# file_name = 'results_NET_A+_H..A+_L..V+_V..K+_K..A+_I_SCI-CASE_BNS_WF_tf2_tidal_INJS-PER-ZBIN_30000.npy'\n",
    "output_path = \"plots/\"\n",
    "output_file_name = \"sky_localisation.pdf\"\n",
    "\n",
    "# loading\n",
    "results = np.load(data_path + input_file_name)\n",
    "\n",
    "z = results[:, 0]\n",
    "snr = results[:, 1]\n",
    "skyarea = results[:, 6]\n",
    "\n",
    "# plotting\n",
    "good_colour = \"#e84393\"\n",
    "v_good_colour = \"#00b894\"\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "fig, axs = plt.subplots(\n",
    "    2, 1, sharex=True, figsize=(8, 12), gridspec_kw=dict(hspace=0.05)\n",
    ")\n",
    "\n",
    "axs[0].axhspan(SNR_THRESHOLD_HI, max(snr), color=v_good_colour, alpha=0.5)\n",
    "axs[0].axhspan(\n",
    "    SNR_THRESHOLD_LO, SNR_THRESHOLD_HI, color=good_colour, linestyle=\"-\", alpha=0.5\n",
    ")\n",
    "axs[0].loglog(z, snr, \",\")\n",
    "axs[0].set(ylabel=r\"SNR, $\\rho$\", ylim=(None, max(snr)))\n",
    "\n",
    "if max(skyarea) > TOTAL_SKY_AREA_SQR_DEG:\n",
    "    axs[1].axhspan(TOTAL_SKY_AREA_SQR_DEG, max(skyarea), color=(0, 0, 0, 0.5))\n",
    "axs[1].loglog(\n",
    "    z[snr < SNR_THRESHOLD_LO], skyarea[snr < SNR_THRESHOLD_LO], \",\", color=\"#0984e3\"\n",
    ")\n",
    "axs[1].loglog(\n",
    "    z[np.logical_and(SNR_THRESHOLD_LO < snr, snr < SNR_THRESHOLD_HI)],\n",
    "    skyarea[np.logical_and(SNR_THRESHOLD_LO < snr, snr < SNR_THRESHOLD_HI)],\n",
    "    \",\",\n",
    "    color=good_colour,\n",
    ")\n",
    "axs[1].loglog(\n",
    "    z[SNR_THRESHOLD_HI < snr], skyarea[SNR_THRESHOLD_HI < snr], \",\", color=v_good_colour\n",
    ")\n",
    "axs[1].axhline(\n",
    "    EM_FOLLOWUP_SKY_AREA_SQR_DEG, color=\"k\", linewidth=1, label=\"follow-up\\nthreshold\"\n",
    ")\n",
    "axs[1].axhline(\n",
    "    MOON_SKY_AREA_SQR_DEG, color=\"k\", linestyle=\"--\", linewidth=1, label=\"Moon\"\n",
    ")\n",
    "axs[1].set(\n",
    "    ylabel=r\"$\\Omega_{90}$ / deg${}^2$\", xlabel=\"redshift, z\", ylim=(None, max(skyarea))\n",
    ")\n",
    "axs[1].legend(handlelength=1)\n",
    "\n",
    "fig.suptitle(plot_title, y=0.9, verticalalignment=\"baseline\")\n",
    "fig.align_labels()\n",
    "\n",
    "fig.canvas.draw()\n",
    "force_log_grid(axs[0])\n",
    "force_log_grid(axs[1])\n",
    "\n",
    "# fig.savefig(output_path + output_file_name, bbox_inches='tight')\n",
    "# # .pdf is 5 MB uncompressed, save as a .jpg for messaging (.png has a transparent background)\n",
    "# fig.savefig(output_path + output_file_name.replace('.pdf', '.jpg'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: detail contour plot to examine sky localisation, contours from 10, 30, and 100\n",
    "# to-do: invent some quantitative metrics to judge sky localisation\n",
    "results = InjectionResults(\n",
    "    \"results_NET_A+_H..A+_L..V+_V..K+_K..A+_I_SCI-CASE_BBH_WF_lal_bbh_IMRPhenomHM_INJS-PER-ZBIN_250000.npy\"\n",
    ")\n",
    "seed = 12345\n",
    "resampled_results = resample_redshift_cosmologically_from_results(results, seed=seed)\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "fig, axs = plt.subplots(\n",
    "    2,\n",
    "    6,\n",
    "    figsize=(20, 3.75),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    gridspec_kw=dict(wspace=0.05, hspace=0.05),\n",
    ")\n",
    "for ax in axs.flatten():\n",
    "    ax.set(xscale=\"log\", yscale=\"log\")\n",
    "# axs[0].loglog(results.redshift, results.snr, ',')\n",
    "# axs[1].loglog(results.redshift, results.sky_area_90, ',')\n",
    "# axs[0].loglog(resampled_results[:,0], resampled_results[:,1], ',')\n",
    "# axs[1].loglog(resampled_results[:,0], resampled_results[:,6], ',')\n",
    "add_measurement_errs_CDFs_to_axs(\n",
    "    axs,\n",
    "    resampled_results,\n",
    "    40,\n",
    "    \"r\",\n",
    "    \"-\",\n",
    "    \"test\",\n",
    "    normalise_count=True,\n",
    "    threshold_by_SNR=True,\n",
    "    contour=True,\n",
    ")\n",
    "plt.show()\n",
    "# what's happening at large z > 30? not quite a shelf but still somewhat shrimp-like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following B&S2022 Section 4A: *Incorporation of redshift-dependent merger rates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tag = \"GWTC3\"\n",
    "if norm_tag == \"GWTC3\":\n",
    "    normalisations = (GWTC3_MERGER_RATE_BNS, GWTC3_MERGER_RATE_BBH)\n",
    "elif norm_tag == \"GWTC2\":\n",
    "    normalisations = (GWTC2_MERGER_RATE_BNS, GWTC2_MERGER_RATE_BBH)\n",
    "\n",
    "# # to use on saved results or compare to B&S2022\n",
    "zmin, zmax = 1e-2, 50\n",
    "# to compare to Ssohrab's email\n",
    "# zmin, zmax = 1.7e-3, 50\n",
    "\n",
    "# first, check that can recover a 10-year observation merger population (see From_Ssohrab_BNS_population_vs_redshift.png)\n",
    "xlim, ylim = (zmin, zmax), (5e-1, 8e5)\n",
    "z_arr = np.geomspace(*xlim, 100)\n",
    "bns_rate_arr = np.array(\n",
    "    parallel_map(\n",
    "        lambda z: merger_rate_bns(z, normalisation=normalisations[0]),\n",
    "        z_arr,\n",
    "        parallel=True,\n",
    "    )\n",
    ")\n",
    "bbh_rate_arr = np.array(\n",
    "    parallel_map(\n",
    "        lambda z: merger_rate_bbh(z, normalisation=normalisations[1]),\n",
    "        z_arr,\n",
    "        parallel=True,\n",
    "    )\n",
    ")\n",
    "bns_obs_rate_arr = np.array(\n",
    "    parallel_map(\n",
    "        lambda z: merger_rate_in_obs_frame(\n",
    "            merger_rate_bns, z, normalisation=normalisations[0]\n",
    "        ),\n",
    "        z_arr,\n",
    "        parallel=True,\n",
    "    )\n",
    ")\n",
    "bbh_obs_rate_arr = np.array(\n",
    "    parallel_map(\n",
    "        lambda z: merger_rate_in_obs_frame(\n",
    "            merger_rate_bbh, z, normalisation=normalisations[1]\n",
    "        ),\n",
    "        z_arr,\n",
    "        parallel=True,\n",
    "    )\n",
    ")\n",
    "# merger rate R(z) in (count)/yr, multiply by number of years to get (count)\n",
    "observation_time_in_years = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The units of $R(z)$ are $\\frac{[\\text{count}]}{\\text{year}\\, [\\text{redshift}]}$ since the yearly number of sources in $(z_0,z_0+\\Delta z)$ is $\\int_{z_0}^{z_0+\\Delta z} R(z) \\text{d}z$ not $R_i=R(z^\\ast)$ where $z^\\ast$ is any point in, e.g. the geometric mean of, $(z_0,z_0+\\Delta z)$.\n",
    "\n",
    "Instead of $p_i = \\frac{R_i}{\\sum_j R_j}$, therefore, I use $q_i = \\frac{R_i \\Delta z_i}{\\sum_j R_j \\Delta z_j} \\approx \\frac{R_i \\Delta z_i}{\\int R(z) \\text{d}z}$ where $\\Delta z_i$ is the width of the bin as the probability of selecting the index $i$ of each bin. For a ten-year observation period $\\tau=10$, I draw $\\tau \\int R(z) \\text{d}z$ samples from the probability distribution which means that the expected number of samples in each bin is $\\frac{\\tau R_i \\Delta z_i \\int R(z) \\text{d}z}{\\sum_j R_j \\Delta z_j}\\approx \\tau R_i \\Delta z_i\\approx \\tau\\int_{z_0}^{z_0+\\Delta z} R(z) \\text{d}z$ as it should be (instead of $\\frac{\\tau R_i \\int R(z) \\text{d}z}{\\sum_j R_j}$).\n",
    "\n",
    "Plotting $\\tau R_i \\Delta z_i$ against $z$ is *not* the same as plotting $\\tau R(z)$ against $z$ since, as the width of each bin decreases, $\\tau R_i \\Delta z_i \\rightarrow 0$. For example, for a uniform merger rate, splitting a bin in half means that there are now two bins next to each other with half the initial number of sources. On a plot, however, it just appears that the number of sources decreased since the split bins look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replicating section 4A\n",
    "# N in B&S2022, containing ~2000 injections from uniform sampler (varying due to randomness and linear sampling)\n",
    "num_subzbin = 150  # 150 in B&S2022 but 75 in Ssohrab's email.\n",
    "subzbin = list(\n",
    "    zip(\n",
    "        np.geomspace(zmin, zmax, num_subzbin + 1)[:-1],\n",
    "        np.geomspace(zmin, zmax, num_subzbin + 1)[1:],\n",
    "    )\n",
    ")\n",
    "# using geometric mean to find the log-centre of each bin\n",
    "subzbin_centres = [gmean(zbin) for zbin in subzbin]\n",
    "subzbin_widths = np.array([zbin[1] - zbin[0] for zbin in subzbin])\n",
    "# using rate in the source frame like B&S2022, to-do: does this make sense if we're coutning observations? are we trying to emulate using a cosmological sampler originally?\n",
    "\n",
    "merger_rate_fns = (\n",
    "    lambda z: merger_rate_bns(z, normalisation=normalisations[0]),\n",
    "    lambda z: merger_rate_in_obs_frame(\n",
    "        merger_rate_bns, z, normalisation=normalisations[0]\n",
    "    ),\n",
    "    lambda z: merger_rate_bbh(z, normalisation=normalisations[1]),\n",
    "    lambda z: merger_rate_in_obs_frame(\n",
    "        merger_rate_bbh, z, normalisation=normalisations[1]\n",
    "    ),\n",
    ")\n",
    "# R_i in B&S2022\n",
    "subzbin_merger_rates = [\n",
    "    np.array(parallel_map(merger_rate, subzbin_centres, parallel=True))\n",
    "    for merger_rate in merger_rate_fns\n",
    "]\n",
    "# p_i in B&S2022\n",
    "subzbin_probabilities = [rate / rate.sum() for rate in subzbin_merger_rates]\n",
    "# q_i, weighting by width of each bin to estimate the actual number of mergers (see latex'd notes)\n",
    "subzbin_weighted_probs = [\n",
    "    rate * subzbin_widths / np.sum(rate * subzbin_widths)\n",
    "    for rate in subzbin_merger_rates\n",
    "]\n",
    "\n",
    "# \"the desired [total, cosmological] number\" of mergers over 10 years, integrating the merger rate in the *source* frame over the redshift range\n",
    "observation_time_in_years = 10\n",
    "num_draws = [\n",
    "    int(observation_time_in_years * quad(merger_rate, zmin, zmax)[0])\n",
    "    for merger_rate in merger_rate_fns\n",
    "]\n",
    "drawn_indicies = [\n",
    "    rv_discrete(values=(range(num_subzbin), pdf)).rvs(size=num_draws[i])\n",
    "    for (i, pdf) in enumerate(subzbin_probabilities)\n",
    "]\n",
    "drawn_weighted_inds = [\n",
    "    rv_discrete(values=(range(num_subzbin), pdf)).rvs(size=num_draws[i])\n",
    "    for (i, pdf) in enumerate(subzbin_weighted_probs)\n",
    "]\n",
    "# n_i in B&S2022: sample i with probability p_i \"up to the desired [total, cosmological] number\" of mergers over 10 years\n",
    "subzbin_num_samples = [\n",
    "    np.array([np.sum(indicies == i) for i in range(num_subzbin)])\n",
    "    for indicies in drawn_indicies\n",
    "]\n",
    "subzbin_num_samples_weighted = [\n",
    "    np.array([np.sum(indicies == i) for i in range(num_subzbin)])\n",
    "    for indicies in drawn_weighted_inds\n",
    "]\n",
    "\n",
    "# cumulative merger rate (goes from zero but this should change little), pass source frame to detection_rate_limit\n",
    "cumulative_merger_rate_bns_obs = np.array(\n",
    "    parallel_map(\n",
    "        lambda z: detection_rate_limit(merger_rate_fns[0], z),\n",
    "        subzbin_centres,\n",
    "        parallel=True,\n",
    "    )\n",
    ")\n",
    "cumulative_merger_rate_bbh_obs = np.array(\n",
    "    parallel_map(\n",
    "        lambda z: detection_rate_limit(merger_rate_fns[2], z),\n",
    "        subzbin_centres,\n",
    "        parallel=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour1, colour2 = \"r\", \"b\"\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "fig, axs = plt.subplots(\n",
    "    2, 1, sharex=True, figsize=(7.8, 12), gridspec_kw=dict(hspace=0.1)\n",
    ")\n",
    "\n",
    "axs[0].loglog(\n",
    "    z_arr,\n",
    "    observation_time_in_years * bns_rate_arr,\n",
    "    color=colour1,\n",
    "    label=\"BNS source frame\",\n",
    ")\n",
    "axs[0].loglog(\n",
    "    z_arr,\n",
    "    observation_time_in_years * bns_obs_rate_arr,\n",
    "    linestyle=\"--\",\n",
    "    color=colour1,\n",
    "    label=\"BNS observer frame\",\n",
    ")\n",
    "axs[0].loglog(\n",
    "    z_arr,\n",
    "    observation_time_in_years * bbh_rate_arr,\n",
    "    color=colour2,\n",
    "    label=\"BBH source frame\",\n",
    ")\n",
    "axs[0].loglog(\n",
    "    z_arr,\n",
    "    observation_time_in_years * bbh_obs_rate_arr,\n",
    "    linestyle=\"--\",\n",
    "    color=colour2,\n",
    "    label=\"BBH observer frame\",\n",
    ")\n",
    "axs[0].set(ylabel=r\"10-year count per redshift, $10\\, R(z)$\", xlim=xlim, ylim=ylim)\n",
    "axs[0].set_ylabel(\"10-year count per redshift,\\n\" + r\"$10\\, R(z)$\", fontsize=20)\n",
    "axs[0].legend()\n",
    "axs[0].set_title(f\"{norm_tag}–normalised merger population per redshift\")\n",
    "force_log_grid(axs[0])\n",
    "\n",
    "# where='mid' makes steps occur half (presumably arithmetic average) way between x values\n",
    "axs[1].step(\n",
    "    subzbin_centres,\n",
    "    subzbin_num_samples_weighted[0],\n",
    "    color=colour1,\n",
    "    where=\"mid\",\n",
    "    linewidth=1,\n",
    "    label=\"BNS source frame\",\n",
    ")\n",
    "axs[1].step(\n",
    "    subzbin_centres,\n",
    "    subzbin_num_samples_weighted[1],\n",
    "    color=colour1,\n",
    "    linestyle=\"--\",\n",
    "    where=\"mid\",\n",
    "    linewidth=0.5,\n",
    "    label=\"BNS observer frame\",\n",
    ")\n",
    "axs[1].step(\n",
    "    subzbin_centres,\n",
    "    subzbin_num_samples_weighted[2],\n",
    "    color=colour2,\n",
    "    where=\"mid\",\n",
    "    linewidth=1,\n",
    "    label=\"BBH source frame\",\n",
    ")\n",
    "axs[1].step(\n",
    "    subzbin_centres,\n",
    "    subzbin_num_samples_weighted[3],\n",
    "    color=colour2,\n",
    "    linestyle=\"--\",\n",
    "    where=\"mid\",\n",
    "    linewidth=0.5,\n",
    "    label=\"BBH observer frame\",\n",
    ")\n",
    "axs[1].loglog(\n",
    "    subzbin_centres,\n",
    "    cumulative_merger_rate_bns_obs,\n",
    "    color=\"k\",\n",
    "    linewidth=2,\n",
    "    label=\"cumulative BNS obs.\",\n",
    ")\n",
    "axs[1].loglog(\n",
    "    subzbin_centres,\n",
    "    cumulative_merger_rate_bbh_obs,\n",
    "    color=\"k\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"cumulative BBH obs.\",\n",
    ")\n",
    "axs[1].set(xscale=\"log\", yscale=\"log\")\n",
    "# axs[1].loglog(subzbin_centres, subzbin_num_samples[0], '.', color='r')\n",
    "axs[1].set(xlabel=\"redshift, $z$\", xlim=xlim, ylim=ylim)\n",
    "# colour is red to signify that this is a problematic axis that changes with the choice of bin width\n",
    "axs[1].set_ylabel(\n",
    "    \"10-year count in bin,\\n\"\n",
    "    + r\"$\\frac{R_i \\Delta z_i}{\\sum_k R_k \\Delta z_k}10 \\int R(z) \\mathrm{d}z\\approx10\\,R_i \\Delta z_i$\",\n",
    "    fontsize=20,\n",
    "    color=\"r\",\n",
    ")\n",
    "axs[1].legend()\n",
    "axs[1].set_title(f\"{norm_tag}–normalised merger population sampled\")\n",
    "force_log_grid(axs[1])\n",
    "axs[1].set_yticks([10**i for i in range(6)], color=\"r\")\n",
    "axs[1].set_yticklabels([rf\"$10^{i}$\" for i in range(6)], color=\"r\")\n",
    "axs1_twin = axs[1].twinx()\n",
    "axs1_twin.set_ylabel(\"10-year cumulative count\", fontsize=20, color=\"k\")\n",
    "axs1_twin.set(xlim=xlim, ylim=ylim, yscale=\"log\")\n",
    "\n",
    "fig.align_labels()\n",
    "fig.savefig(f\"plots/ten-year_population_combined_{norm_tag}.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling uniformly n_i times from the saved data in the subzbin with index i\n",
    "file = \"data_redshift_snr_errs_sky-area/results_NET_A+_H..A+_L..K+_K..A+_I..ET_E_SCI-CASE_BNS_WF_tf2_tidal_INJS-PER-ZBIN_60000.npy\"\n",
    "colours = (\"b\", \"r\", \"k\")\n",
    "big_zbins_end_points = (0.02, 0.5, 1, 2, 4, 10, 50)\n",
    "\n",
    "results = np.load(file)\n",
    "resampled_results = resample_redshift_cosmologically_from_results(\n",
    "    results, \"BNS\"\n",
    ")  # , print_progress=True, print_samples_with_replacement=True)\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "fig, axs = plt.subplots(\n",
    "    2, 1, sharex=True, sharey=True, figsize=(8, 8), gridspec_kw=dict(hspace=0.05)\n",
    ")\n",
    "axs[0].loglog(\n",
    "    results[:, 0],\n",
    "    results[:, 1],\n",
    "    \",\",\n",
    "    color=colours[0],\n",
    "    label=f\"log-uniformly sampled results: {len(results)} points\",\n",
    ")\n",
    "for endpoint in big_zbins_end_points:\n",
    "    for ax in axs:\n",
    "        ax.axvline(endpoint, color=\"k\", linewidth=0.5)\n",
    "axs[1].loglog(\n",
    "    resampled_results[:, 0],\n",
    "    resampled_results[:, 1],\n",
    "    \",\",\n",
    "    color=colours[1],\n",
    "    label=f\"cosmo. re-sampled results: {len(resampled_results)} points\",\n",
    ")\n",
    "axs1_twin = axs[1].twinx()\n",
    "# using observed rate\n",
    "axs1_twin.loglog(\n",
    "    subzbin_centres,\n",
    "    cumulative_merger_rate_bbh_obs,\n",
    "    color=colours[2],\n",
    "    linewidth=2,\n",
    "    label=\"cumulative BNS observed\",\n",
    ")\n",
    "axs1_twin.set_ylabel(\"10-year cumulative count\", color=colours[2])\n",
    "axs1_twin.set(xlim=xlim, ylim=ylim, yscale=\"log\")\n",
    "\n",
    "axs[0].set(ylabel=\"integrated SNR\")\n",
    "axs[0].set_title(\n",
    "    f\"re-sampling redshift cosmologically\\n{file_name_to_multiline_readable(file)}\"\n",
    ")\n",
    "axs[1].set(xlabel=\"redshift, z\", xlim=(2e-2, 5e1))\n",
    "axs[1].set_ylabel(\"integrated SNR\", color=colours[1])\n",
    "axs[1].yaxis.label.set_color(colours[1])\n",
    "axs[1].tick_params(axis=\"y\", which=\"both\", colors=colours[1])\n",
    "\n",
    "axs[0].legend(\n",
    "    labels=axs[0].get_legend_handles_labels()[1],\n",
    "    handles=[mlines.Line2D([], [], color=colours[0])],\n",
    "    loc=\"lower right\",\n",
    ")\n",
    "axs[1].legend(\n",
    "    labels=(\n",
    "        *axs[1].get_legend_handles_labels()[1],\n",
    "        *axs1_twin.get_legend_handles_labels()[1],\n",
    "    ),\n",
    "    handles=[\n",
    "        mlines.Line2D([], [], color=colours[1]),\n",
    "        mlines.Line2D([], [], color=colours[2]),\n",
    "    ],\n",
    "    loc=\"lower right\",\n",
    ")\n",
    "\n",
    "fig.align_labels()\n",
    "\n",
    "for ax in axs:\n",
    "    force_log_grid(ax)\n",
    "\n",
    "fig.savefig(\n",
    "    \"plots/cosmological_resampling_from_log-uniform_BNS.png\", bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking task files, e.g. their injection loss, before merging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "path = \"data_redshift_snr_errs_sky-area/\"\n",
    "files = glob.glob(path + \"*TASK*\")\n",
    "files = sorted(files, key=lambda f: int(f.replace(\"_TASK_\", \".npy\").split(\".npy\")[1]))\n",
    "ids = [int(f.replace(\"_TASK_\", \".npy\").split(\".npy\")[1]) for f in files]\n",
    "missing_ids = [i for i in range(1, 2041) if i not in ids]\n",
    "print(missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_files = np.array(files)[sorted(np.random.randint(1, len(files), 1000))]\n",
    "initial_num_injs, final_num_injs, redshifts = [], [], []\n",
    "for f in sampled_files:\n",
    "    results = InjectionResults(f)\n",
    "    initial_num_injs.append(results.num_injs)\n",
    "    final_num_injs.append(results.remaining_num_injs)\n",
    "    redshifts.append(results.redshift.mean())\n",
    "initial_num_injs, final_num_injs = np.array(initial_num_injs), np.array(final_num_injs)\n",
    "losses = (initial_num_injs - final_num_injs) / initial_num_injs\n",
    "sampled_science_cases = np.array(\n",
    "    [f.replace(\"_SCI-CASE_\", \"_WF_\").split(\"_WF_\")[1] for f in sampled_files]\n",
    ")\n",
    "sampled_inds = np.array(\n",
    "    [int(f.replace(\"_TASK_\", \".npy\").split(\".npy\")[1]) for f in sampled_files]\n",
    ")\n",
    "\n",
    "# sort by losses\n",
    "sort_inds = losses.argsort()\n",
    "losses_sorted, sampled_files_sorted = losses[sort_inds], sampled_files[sort_inds]\n",
    "\n",
    "\n",
    "def print_loss(print_inds):\n",
    "    for i, f in enumerate(sampled_files_sorted[print_inds[0] : print_inds[1]]):\n",
    "        print(f'{losses_sorted[print_inds[0] + i]:.2%}: {f.replace(path, \"\")}')\n",
    "\n",
    "\n",
    "total_sampled_loss = (sum(initial_num_injs) - sum(final_num_injs)) / sum(\n",
    "    initial_num_injs\n",
    ")\n",
    "print(f\"total sampled loss: {total_sampled_loss:.2%}\")\n",
    "print(\"sample\\n- - -\")\n",
    "print_loss((0, 3))\n",
    "print(\"...\")\n",
    "print_loss((-3, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_global_fontsize(14)\n",
    "fig, ax = plt.subplots()\n",
    "xlim = 0, 1024\n",
    "(line_BBH,) = ax.plot(\n",
    "    sampled_inds[sampled_science_cases == \"BBH\"],\n",
    "    losses[sampled_science_cases == \"BBH\"],\n",
    "    \".\",\n",
    "    label=\"BBH\",\n",
    ")\n",
    "(line_BNS,) = ax.plot(\n",
    "    sampled_inds[sampled_science_cases == \"BNS\"] - 1024,\n",
    "    losses[sampled_science_cases == \"BNS\"],\n",
    "    \".\",\n",
    "    label=\"BNS\",\n",
    ")\n",
    "ax.axhline(\n",
    "    0.05, color=\"r\", label=f\"expected loss from\\nill-conditioned FIM: ~{0.05:.0%}\"\n",
    ")\n",
    "ax.axhline(\n",
    "    total_sampled_loss, color=\"b\", label=f\"total sampled loss: {total_sampled_loss:.2%}\"\n",
    ")\n",
    "ax.set_yticklabels([f\"{tick:.0%}\" for tick in ax.get_yticks()])\n",
    "ax.set(\n",
    "    xlabel=\"task index (weakly correlated with redshift bin)\",\n",
    "    ylabel=\"fraction of injections lost\",\n",
    "    title=f\"sampled losses from {len(sampled_files)} tasks\",\n",
    "    xlim=xlim,\n",
    "    yscale=\"log\",\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    [f\"{tick:.0f}\" for tick in ax.get_xticks()], color=line_BBH.get_color()\n",
    ")\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "ax.grid()\n",
    "\n",
    "ax_twin = ax.twiny()\n",
    "ax_twin.set(xlim=xlim)\n",
    "ax_twin.set_xticklabels(\n",
    "    [f\"{tick + 1024:.0f}\" for tick in ax_twin.get_xticks()], color=line_BNS.get_color()\n",
    ")\n",
    "\n",
    "plt.savefig(\"plots/injection_losses.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: figure out why tasks 54 and 154 worked on the 2nd try but not the first, try without debug on for the third run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = glob.glob(\"data_redshift_snr_errs_sky-area/*INJS-PER-ZBIN_60000.npy\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = glob.glob(\"data_redshift_snr_errs_sky-area/*\")\n",
    "z = [f for f in z if \"TASK\" not in f]\n",
    "injs = [int(f.replace(\"_INJS-PER-ZBIN_\", \".npy\").split(\".npy\")[1]) for f in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in NET_DICT_LIST[0][\"nets\"]:\n",
    "    label = network_spec_to_net_label(net, styled=True)\n",
    "    #     print(label)\n",
    "    files = glob.glob(f\"data_redshift_snr_errs_sky-area/results_NET_{label}_SCI-CASE*\")\n",
    "    #     print(files)\n",
    "    for file in files:\n",
    "        print(file_name_to_multiline_readable(file))\n",
    "        x = np.load(file)\n",
    "        print(\n",
    "            f\"results files shape: {x.shape}, meaning {x.shape[0]/6/1e3:.1f}k per major zbin\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  To-do list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: stop CE alone being ill-conditioned, seems to work for BBH numeric wf --> get latest patch from Borhanian\n",
    "# to-do: fix BNS numerical waveform, current error \"numpy.linalg.LinAlgError: Array must not contain infs or NaNs\" with HLVKI+ --> see above (still dropping BNS numerical injections but no longer the same error - 20220421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: change sigmoid fits to global optimisation to avoid needing the initial guesses in B&S2022, this is not possible with scipy, need to write my own or source a global curve fitting (apparantly a non-trivial problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: refactor two plotting functions\n",
    "# to-do: tidy up refactoring, reduce total number of arguments with unpacking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In fact, we see that the three generations (A+, Voyager, and NG) are qualitatively different\n",
    "with respect to every metric used in this study.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
